## 摘要

## 绪论（1～5）

### 研究背景及研究意义

### 研究现状

### 工作内容/目标

### 论文结构

## 相关原理和技术（6～12）

- 图像分类任务
  - 卷积神经网络
  - 视觉transformer
  - 预训练视觉模型
- 文本分类任务

  - self-attention
  - 预训练语言模型
- 多模态分类任务

  - 多模态transformer
  - late/early fusion

## 设计与实现（13～25）

在介绍具体的方法之前，先对一些下文常用到的符号进行定义

|         $X_t$          |             文本输入             |
| :--------------------: | :------------------------------: |
|         $X_i$          |             图像输入             |
| $Y_t(\overline {Y_t})$ | 文本对应的标签（文本模型的预测） |
| $Y_i(\overline {Y_i})$ | 图像对应的标签（图像模型的预测） |
|   $Y(\overline {Y})$   |   综合文本，图像的标签（同上）   |
|         $M_t$          |        用于编码文本的模型        |
|         $M_i$          |        用于编码图像的模型        |
|          $M$           |            多模态模型            |
|         $H_t$          |        文本模态的特征向量        |
|         $H_i$          |        图像模态的特征向量        |
|          $H$           | 融合后的特征向量（全局特征向量） |
|         $L_t$          |         文本模态的Logits         |
|         $L_i$          |         图像模态的Logits         |
|          $L$           |           多模态Logits           |

### 数据预处理

- 文本预处理

  本文选取的数据集中的文本大多数为英语，因此首先将其全部转为小写，在去除收尾的空白符号（换行，制表符），在直接使用空格进行分词。分词之后，利用BertTokenizer进行转化为词表中的索引，并在首部加上【CLS】占位符，尾部加上【SEP】占位符。记为$X_t$

- 图像预处理

  为了适配模型，首先将图像的大小调整为256\*256，在截取图像中间的224\*224个像素点作为最终的图像。由于模型大多数都在Imagenet上预训练，因此需要将输入的图片按照Imagenet数据集的均值和方差进行归一化。具体来说。图像为RGB三通道，故对每一个通道都进行归一化。记为$X_i$
  $$
  X_i \leftarrow \frac{X_i - mean}{std}
  $$
   

### 实现细节

Python + Pytorch + Huggingface + Timm(Pytorch-image-models) + streamlit

AdamW 

Tesla T4

更具体的超参数列在附录中

### 基于视觉transformer的图像情感分类

​	对于图像情感分类，这里采用以Vision Transformer（ViT）为代表的一系列视觉transformer。视觉transformer最初提出是用于图像分类，目标检测这类视觉任务中，即识别出一张图像中的物体，本文将其称为图像的浅层信息。但情感识别任务需要判断出图像中的物体所蕴含的情感，本文将其称为图像的深层信息。

​	具体来说，给定图像输入$X_i$
$$
\overline{L_i} = MLP(ViT(X_i)) \\
\overline{Y_i} = \arg \max(Softmax(\overline{L_i})
$$
​	其中，$MLP$为线性层，用于将ViT编码后的隐变量变换为输出维度的向量，输出维度即为情感种类个数，通常为二或者三。

​	训练过程中用交叉熵作为损失函数
$$
Loss = CrossEntrophy(\overline{L_i}, Y_i)
$$


- 贴图

### 基于BERT的文本情感分类

​	对于文本情感分类任务，本文采用BERT。

​	具体来说，给定文本输入$X_t$
$$
\overline{L_t} = MLP(BERT(X_t)) \\
\overline{Y_t} = \arg \max(Softmax(\overline{L_t})
$$
​	其中，$MLP$为线性层，用于将BERT编码后的隐变量变换为输出维度的向量，输出维度即为情感种类个数，通常为二或者三。

​	训练过程中同样使用交叉熵作为损失函数
$$
Loss = CrossEntrophy(\overline{L_t}, Y_t)
$$




- 贴图

### 多模态情感分类

​	对于多模态情感分类任务，主要分为两种处理方式。1）利用不同的模型对不同模态的输入进行编码，再将编码后的模态特征向量进行特征融合，最后将全局特征输入给一个分类器进行情感分类。2）将不同模态的输入拼接成一个全局输入，输入给一个模型进行编码，最后直接将特征向量输入个一个分类器。下面具体介绍这两种方法。

- 不同模态输入单独编码

  ![avatar](figure1.pdf)

  该模型的具体结构见图，对于图像/文本的编码方式在前面两个部分已经具体介绍过了，这里只从特征融合这一步开始具体介绍。首先将$H_i,H_t$拼接为全局特征$H$，即
  $$
  H = concatenate(H_i,H_t)
  $$
  再将$H$输入给一个线性分类层得到logits，再通过一个softmax层得到概率分布
  $$
  \overline{L} = MLP(H) \\
  \overline{Y} = \arg \max {Softmax(\overline{L})}
  $$
  

- 不同模态输入统一编码

  ![avatar](figure2.pdf)

  该模型的具体结构见图，首先利用一个图片编码器提取出图像的特征。具体来说，拿Resnet152举例。将倒数第二层的特征图（feature map）进行池化后，变换成若干个1*1的特征图。将这些特征图拼接到文本模态输入的后面，作为全局输入。将其输入到BERT当中，利用[CLS]的特征向量进行分类。数学公式如下
  $$
  X_i \leftarrow Image\_Encoder(X_i)\\
  X = Concatenate(X_t,X_i)\\
  H = BERT(X)\\
  \overline{L} = MLP(H)\\
  \overline{Y} = \arg \max Softmax(\overline{L})
  $$
  

- 贴图

### 实验结果

#### 评价指标

- Accuracy（准确率）：分类正确个数/总数
- AUROC（操作特征曲线下的面积）：

#### 图像情感分类

|          | Vanilla Resnet152 | Resnet152 | Vanilla Vit | Vit   | Vanilla Swint | Swint     | Vanilla TNT | TNT   | Vanilla PiT | PiT   |
| -------- | ----------------- | --------- | ----------- | ----- | ------------- | --------- | ----------- | ----- | ----------- | ----- |
| Accuracy | 56.00             | 67.50     | 58.75       | 66.75 | 59.25         | 67.75     | 58.75       | 66.75 | 58.50       | 66.00 |
| AUROC    | **66.21**         | 78.89     | 65.45       | 81.19 | 61.02         | **81.79** | 64.61       | 78.94 | 62.85       | 80.42 |

#### 文本情感分类

|          | BERT |
| -------- | ---- |
| Accuracy |      |
| AUROC    |      |



#### 多模态情感分类

|          | Resnet152 | Vanilla Vit | Vit       | Vanilla Swint | Swint | Vanilla TNT | TNT   | Vanilla PiT | PiT   |
| -------- | --------- | ----------- | --------- | ------------- | ----- | ----------- | ----- | ----------- | ----- |
| Accuracy | 60.40     | 57.4        | 61.60     | 56.60         | 60.00 | 55.40       | 59.40 | 58.00       | 60.60 |
| AUROC    | 62.90     | 62.78       | **66.80** | 62.86         | 66.20 | 62.82       | 65.37 | 63.28       | 65.66 |

|          | Resnet152 | Vit   | Swint | TNT   | PiT   |
| -------- | --------- | ----- | ----- | ----- | ----- |
| Accuracy | 60.60     | 62.40 | 61.40 | 63.80 | 62.60 |
| AUROC    | 65.57     | 68.79 | 67.40 | 66.78 | 66.92 |

### Ablation studies

- 是否需要预训练

  对于Transformer模型来说，预训练基本上是一个默认的设定，近年来大火的BERT，Roberta，T5等等都是在各种庞大的语料库上先预训练，再用于下游任务当中。Vision transformer也一样，上述的ViT，Swint，TnT，PiT都是在ImageNet数据集上预训练后迁移到下游的视觉任务当中。一个自然的问题是，一个未预训练过的Vision Transformer的效果如何（这里特指和未预训练过的卷积神经网络进行比较）

  |          | Resnet152 | Vanilla Resnet152 | Vanilla ViT | Vanilla Swint | Vanilla TNT | Vanilla PiT |
  | :------: | :-------: | :---------------: | :---------: | :-----------: | :---------: | :---------: |
  | Accuracy |   67.50   |       56.00       |    58.75    |   **59.25**   |    58.75    |    58.50    |
  |  AUROC   |   78.89   |     **66.21**     |    65.45    |     61.02     |    64.61    |    62.85    |

  |          |  ViT  | Vanilla ViT | Vanilla Swint | Vanilla TNT | Vanilla PiT |
  | :------: | :---: | :---------: | :-----------: | :---------: | :---------: |
  | Accuracy | 61.60 |    57.4     |     56.60     |    55.40    |    58.00    |
  |  AUROC   | 66.80 |    62.78    |     62.86     |    62.82    |    63.28    |

  第一张表格是在MVSA数据集上进行图像情感分类得到的结果，可以看出，Resnet152的AUROC值位列第一，说明预训练对于Transformer的提升要大于对于卷积神经网络的提升。这一点是符合直觉的。因为卷积操作天生就有提取图像特征的能力。

  第二章表格是在Hateful-Meme数据集上进行多模态情感分了得到的结果，可以看出，去除预训练，Vision Transformer的能力会有明显的下降，这证明了预训练的重要性。

  因此得出结论，大规模预训练对于自注意力机制是必不可少的。

- 特征融合方式

  特征融合是多模态学习领域中的一个重点，在之前的模型中，我只用了early-fusion这一种融合，方式。为了探索不同特征融合方式对于分类准确率的影响，这里又对三种特征融合方式进行实验。具体来说有：1）late-fusion，该方法首先将不同模态的特征通过不同的分类器进行分类，再将两个分类结果进行分析得到最终呢的分类结果。2）LTC（Linear-Then-Concatenate），该方法首先将不同模态的特征先通过不同的线性层，变换特征维度，然后再进行拼接。3）STC（Self Attention-Then-Concatenate），该方法首先在不同模态特征上做self-attention，得到新的模态特征，然后再进行拼接并分类。

  

  从结果上来看。。。

- ensemble models

  模型集成是一种常见的用于提升准确率的方式。常规的方法是在训练中同时训练多个模型。但由于计算资源的限制，同时训练多个模型的显存需求远远超过了一张显卡的显存。故这里采用一种简化的模型集成方式，具体来说：依次训练好多个模型，并保存预测结果，按少数服从多数的原则决定集成后的预测结果。这里尝试了不同组合方式下的模型集成效果

  |          | Resnet+Vit+Swint | Vit+Swint+TNT | Resnet+Vit+Swint+Tnt+Pit | ViT   |
  | -------- | ---------------- | ------------- | ------------------------ | ----- |
  | Accuracy | 62.40            | 59.20         | 60.20                    | 61.60 |
  | AUROC    | **67.11**        | 65.49         | 66.25                    | 66.80 |

  该表是在Hateful-Meme数据集上进行多模态情感分类得到的结果。Resnet+ViT+Swint代表用这三种模型预测的结果投票，遵循少数服从多数的原则，另外两个类似。可以看出Resnet+ViT+Swint比单独使用ViT的效果要高，但是Vit+Swint+TNT和Resnet+Vit+Swint+Tnt+Pit却要比ViT更低。一方面说明模型集成这种方法会带来准确率的提升，另一方面说明不恰当的集成方式也有可能会造成准确率的下降。

- Faster R-CNN feature exractor

  在MMBT模型中，本文采用的图像编码器均为卷积神经网络或者视觉Transformer。另外一种常用的特征提取器是Faster R-CNN。该模型主要用于目标检测当中（具体介绍一下），其提取出的特征这里称为Region-Feature，而前文提取的特征统一称为Grid-Feature。

  |          | MMBT-Region |      | MMBT-Grid(Resnet) | MMBT-Grid(ViT) |
  | -------- | ----------- | ---- | ----------------- | -------------- |
  | Accuracy | 64.80       |      | 60.60             | 62.40          |
  | AUROC    | 72.62       |      | 65,57             | 68.79          |

  从结果上来看，MMBT-Region相比于MMBT-Grid有显著的提升，说明Faster R-CNN对于图像特征提取的能力更适用于MMBT这种结构。但是ViT的表现依旧要优于Resnet。

### 错误分析（随机选100个分析）

### 模型部署

- 前端后端

## 模型展示（26～30）

## 附录

- 超参数
- 数据集细节









